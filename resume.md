**Denis Moura**  
**Senior Data Engineer**  
**Email**: drmouradap@gmail.com  
**LinkedIn**: [linkedin.com/in/drmoura](https://www.linkedin.com/in/drmoura/)  
**GitHub**: [github.com/mouradap](https://github.com/mouradap)  

---

### **Summary**  

Senior Data Engineer with 4+ years of experience in designing scalable data pipelines and cloud-native solutions. Skilled in Python, SQL, and AWS, with expertise in migrating on-premises systems to Snowflake and developing ETL pipelines for high-performance analytics. Proven track record of optimizing costs, ensuring data quality, and delivering actionable business insights through data visualization tools. Adept at building robust data platforms and supporting data-driven applications for global organizations. As a leader, often lead by example and work close to the end users to interface the client needs with the engineering team to build robust, client-first applications.

---

### **Skills**  

- **Programming**: Python, SQL, R, JavaScript  
- **Big Data & Warehousing**: Snowflake, BigQuery, Spark, Iceberg, Delta Lake, Kafka  
- **Cloud Platforms**: AWS (S3, Glue, Lambda, Step Functions, Athena), GCP (BigQuery, Storage)  
- **Orchestration & Workflow Management**: Airflow, AWS Step Functions, Prefect  
- **Tools & DevOps**: Terraform, Docker, Kubernetes, Git, GitHub Actions  
- **Data Modeling & Governance**: Data Lakes, Lake Formation, Data Quality Validation  
- **Data Visualization**: Sigma Computing, PowerBI, Looker Studio, Metabase  
- **Methodologies**: Agile/Scrum, Distributed Systems  
- **Exploratory Data Analysis**: Pandas, Polars, Dask  

---

### **Professional Experience**  

**Lead Data Engineer**  
**Dexian Disys** (U.S. based nearshore – remote)  
*2022 – Present*  

- Engineered a scalable data migration platform using **Airflow**, reducing AWS operational costs by **30%** and optimizing ETL processes for datasets exceeding **5TB daily**.  
- Migrated **200+ views** to **Snowflake**, ensuring high data quality and improving query performance by **40%**, supporting critical analytics projects.  
- Created and maintained interactive dashboards using **Spotfire**, **Sigma Computing**, and **PowerBI**, delivering actionable insights to stakeholders across departments.  
- Developed a file upload function for **MS Teams** using **Graph API** and **Airflow**, automating data ingestion into Snowflake with enforced privacy and security controls.  
- Designed and implemented web-based tools for seamless data uploads, used daily across teams for loading gigabytes of data securely and accurately.
- Led a team of 4 experienced Data Engineers, allowing a charismatic example-based leadership. Interfaced final users to data owners and the engineering team to develop robust, client-first data solutions.
- **Technologies**: Python, Airflow, Snowflake, AWS, Terraform, Docker, Kubernetes, Kafka, Git, GitHub Actions, PowerApps  

**Consulting Data Engineer**  
**Freelance** (remote)  
*Continuous*  

- Architected a scalable data lake for a Health Tech startup, enabling secure and seamless data sharing with clients while improving scalability.  
- Developed custom **Airflow operators** for efficient data ingestion and transformation to **Iceberg** format, ensuring data quality across varied sources.  
- **Technologies**: Python, Airflow, Iceberg, AWS, Athena, MySQL, Lake Formation  

**Data Engineer**  
**Varsomics – Hospital Israelita Albert Einstein** (Brazil – remote)  
*2021 – 2022*  

- Led the migration of **70TB** of genomic data to an **AWS Data Lake**, employing **S3**, **Glue**, **Athena**, and **Lake Formation** for data governance and compliance.  
- Enhanced data management protocols through object tagging and governance policies, improving compliance and access controls.  
- Streamlined genomics pipeline monitoring and result analysis by developing dashboards used by internal stakeholders.  
- **Technologies**: Python, AWS Glue, AWS Step Functions, AWS Athena, Terraform, Docker, PySpark  

**Software Engineer**  
**PickCells** (Brazil – Local)  
*2020 – 2021*  

- Developed and maintained a microscopy automation solution, integrating robot movements and camera focus with **Python** and **C libraries**.  
- Spearheaded a cloud migration project, designing a scalable **AWS Data Lake** with automated ETL pipelines using **Airflow**.  
- Led an international project for **COVID-19 network analysis**, applying **network science** methodologies and **deep learning** models.  
- **Technologies**: Python, Airflow, AWS, Kubernetes, Computer Vision, Deep Learning  

---

### **Education**  

**Ph.D. in Applied Biology (Bioinformatics)**  
**Universidade Federal de Pernambuco**  
*2022*  

**Specialization - Data Specialist**  
**SAUTER**  
*2021*  

**M.Sc. in Applied Biology (Neuroscience & Bioinformatics)**  
**Universidade Federal de Pernambuco**  
*2018*  

**B.Sc. in Biology**  
**Universidade Federal de Pernambuco**  
*2015*  

---

### **Certifications**  

- **Airflow**: Astronomer Certification for Apache Airflow Fundamentals (2024), DAG Authoring for Apache Airflow.  
- **AWS**: AWS Partner: Security Governance at Scale (2021), AWS Cloud Practitioner Essentials (2021).  
- **Kafka**: Confluent Apache Kafka Fundamentals (2021).  

---

### **Projects and Achievements**  

- **Data Migration Platform**: Developed a reusable data migration platform using **Airflow**, optimizing cost and performance for a global client by **30%**.  
- **Compliance Data Management Tool**: Designed a **React/NodeJS** web app for monitoring compliance-regulated data in **Snowflake**, enabling version history tracking and secure data management.  
- **Teams-to-Snowflake Integration**: Automated file ingestion from **MS Teams** to Snowflake using **Graph API** and **Airflow**, improving data accessibility and privacy for team-specific schemas.  
- **Genomic Data Lake**: Led the creation of a genomic data lake, improving compliance and reducing query times by **25%**.  
- **Microscopy Automation**: Developed a Python-based automated microscopy solution, enhancing research capabilities and reducing manual effort.  
- **On-premises to AWS Data Lake**: Migrated daily on-premises data to an **AWS Data Lake**, supporting near-real-time ingestion pipelines with **Airflow**.  

---

